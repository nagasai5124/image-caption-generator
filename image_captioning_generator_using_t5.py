# -*- coding: utf-8 -*-
"""image_captioning_generator_using_t5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wx6N-TUCfuoRzip2K9e5MD7TFHusqk6U
"""

!unzip "/content/archive (18).zip"

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
from torch.utils.data import Dataset
import torchvision
import torch.optim as optim
import transformers
import torchvision.models as models
from transformers import T5Tokenizer, T5Model, T5ForConditionalGeneration, T5TokenizerFast
from torch.optim import Adam
from tqdm  import tqdm
from transformers import ViTFeatureExtractor, ViTModel, ViTConfig

import seaborn as sns
import matplotlib.pyplot as plt
import os
from sklearn.preprocessing import LabelEncoder
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split

print(transformers.__version__)

df = pd.read_csv("/content/captions.txt", sep=",", header=None,
                 names=["image", "caption"])

df.head()

image_path=[]
for i in df['image']:
    image_path.append('/content/Images/'+i)

caption=[]
for i in df['caption']:
    caption.append(i)
print(caption[0])

image_path=image_path[1:]
caption=caption[1:]

image_path[0]

from PIL import Image
import PIL
image=image_path[0]
image=Image.open(image)
print(f"caption: {caption[0]}")
image

X_train, X_test, y_train, y_test = train_test_split(image_path,caption, test_size=0.2, random_state=42)

max_len=0
max_len_dis=[]
for i in y_train:
    #print(i)
    #print(len(i.split()))
    max_len_dis.append(len(i.split()))
    if len(i.split())>max_len:
        max_len=len(i.split())
max_len

pos_word={}
c=0
for i in y_train:
    for j in i.split():
        if j not in pos_word:
            pos_word[j]=c
            c+=1
print(len(pos_word))
#pos_word

sns.displot(max_len_dis)

data_transforms = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225])
 ])

org_image_path=X_train[0]
org_img=Image.open(org_image_path)
org_img

print("normalize image")
#plt.imshow(sample_output['image_tensor'].squeeze().permute(1, 2, 0))

model=T5ForConditionalGeneration.from_pretrained('t5-base',return_dict=True,)
tokenizer=T5TokenizerFast.from_pretrained('t5-base')

class imageDataset(Dataset):
    def __init__(self,data,transform,training,lables=None):
        self.data=data
        self.lables=lables
        self.transform=transform
        self.training=training
        self.tokenizer=tokenizer
        self.max_len=max_len


    def __len__(self):
        return len(self.data)


    def __getitem__(self,index):
        image=self.data[index]
        image_1=Image.open(image)
        if self.training:
            caption=str(self.lables[index])
            caption=" ".join(caption.split())
            inputs=self.tokenizer(caption,
                                  max_length=self.max_len,
                                  add_special_tokens=True,
                                  padding='max_length',
                                  pad_to_max_length=True,
                                  truncation=True)
            labels=inputs['input_ids']
            labels[labels==0]=-100
            mask=inputs['attention_mask']
            #print(inputs)
            return {
                'image_tensor':self.transform(image_1),
                'labels':torch.tensor(labels,dtype=torch.long),
                'mask':torch.tensor(mask,dtype=torch.long),
                'decoder_input_ids':torch.tensor(mask,dtype=torch.long)
            }
        else:
            return {
                'image_tensor':self.transform(image_1),
                'labels':torch.tensor(labels,dtype=torch.long),
                'mask':torch.tensor(mask,dtype=torch.long),
                'decoder_input_ids':torch.tensor(mask,dtype=torch.long)

            }

sample_data=imageDataset(data=X_train,transform=data_transforms,training=True,lables=y_train)

sample_output=sample_data.__getitem__(0)

sample_output

train_data_custom=imageDataset(data=X_train,transform=data_transforms,training=True,lables=y_train)
test_data_custom=imageDataset(data=X_test,transform=data_transforms,training=False,lables=y_test)

train_data_loader=DataLoader(dataset=train_data_custom,
                             batch_size=32,
                             num_workers=os.cpu_count(),
                             shuffle=True
)
test_data_loader=DataLoader(dataset=test_data_custom,
                             batch_size=32,
                             num_workers=os.cpu_count(),
                             shuffle=False
)

next(iter(train_data_loader))

class EncoderDecoder(torch.nn.Module):
    def __init__(self,embed_size):
        super(EncoderDecoder,self).__init__()
        resnet=models.resnet50(pretrained=True)

        for parm in resnet.parameters():
            parm.requires_grad_(False)

        modules=list(resnet.children())[:-1]
        self.resnet = nn.Sequential(*modules)
        self.embed = nn.Linear(resnet.fc.in_features, embed_size)

        #decoder: t5
        self.model=T5ForConditionalGeneration.from_pretrained('t5-base',return_dict=True)
        self.tokenizer=T5TokenizerFast.from_pretrained('t5-base')

    def forward(self, images,decoder_inputs_ids,attention_mask,labels):
        features = self.resnet(images)
        features = features.view(features.size(0), -1)
        image_features = self.embed(features).unsqueeze(1)


        output=self.model(
            inputs_embeds=image_features,
            decoder_input_ids=decoder_inputs_ids,
            attention_mask=attention_mask,
            labels=labels
        )
        return output.logits,output.loss

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
max_len=34
training_file=y_train
optimizer=Adam(model.parameters(),lr=0.00001)
epochs=5

test_sample=next(iter(train_data_loader))
test_sample

final_model=EncoderDecoder(embed_size=768)

final_model.forward(test_sample['image_tensor'],test_sample['decoder_input_ids'],test_sample['mask'],test_sample['labels'])



train_loss=0
val_loss=0
train_batch_count=0
val_batch_count=0

for epoch in range(epochs):
    final_model.train()
    for batch in tqdm(train_data_loader):
        #print(batch)
        input_ids=batch['image_tensor'].to(device)
        attention_mask=batch['mask'].to(device)
        labels=batch['labels'].to(device)
        decoder_input_ids=batch['decoder_input_ids'].to(device)
        #input_ids=input_ids.unsqueeze(1)  #[32,224]
        print(input_ids.shape)
        print(decoder_input_ids.shape)
        print(attention_mask.shape)
        print(labels.shape)
        output,loss=final_model(input_ids,decoder_input_ids,attention_mask,labels)
        #print(output)
        optimizer.zero_grad()
        #loss,_=output.loss
        loss.backward()
        optimizer.step()
        train_loss+=loss.item()
        train_batch_count+=1

    print(f"Train Loss: {train_loss/train_batch_count}")

final_model.eval()
for batch in tqdm(test_data_loader):
    inputs_ids=batch['image_tensor'].to(device)
    attention_mask=batch['mask'].to(device)
    labels=batch['labels'].to(device)
    decoder_input_ids=batch['decoder_input_ids'].to(device)
    with torch.no_grad():
        output,loss=model(inputs_ids,decoder_input_ids,attention_mask,labels)
        #loss=output.loss
        val_loss+=loss.item()
        val_batch_count+=1
print(f"Val Loss: {val_loss/val_batch_count}")

